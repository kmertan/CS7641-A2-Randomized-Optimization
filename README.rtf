{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf200
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 This project inspects the differences in performance and run time among Randomized Hill Climbing, Simulated Annealing and Genetic Algorithms on the task of learning neural network weights then again, with MIMIC joining them, on three discrete valued optimization problem domains. \
\
To run the code, run DUMPER.py to separate the data into train, test and validation. Navigate to the ABAGAIL folder in the terminal and run the \'91ant\'92 command (assuming ant is installed on your machine) to build the environment. Next navigate to the test folder from within ABAGAIL and execute any of the python files present using jython -J-cp ../ABAGAIL.jar <file>.py. This assumes you have jython installed on your machine. Executing all of these files will populate the data folder with output from several iterations of each algorithm on each problem. Plots included in the analysis were formed in Plots.R.\
}